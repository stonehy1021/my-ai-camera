import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase
import cv2
import mediapipe as mp
import av
import numpy as np
import time
import queue

# ---------------- 1. ê¸°ë³¸ ì„¤ì • ----------------
st.set_page_config(page_title="mzêµ¬ë„ ì´¬ì˜ê¸° (ì €ì¥ê°€ëŠ¥)", layout="centered")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "snapshot" not in st.session_state:
    st.session_state.snapshot = None

# [ìˆ˜ì • 3] ì¹´ë©”ë¼ ë¨¹í†µ ë°©ì§€ìš© ë²„ì „ í‚¤ (ì¬ì´¬ì˜ ì‹œ ì´ ìˆ«ìë¥¼ ë°”ê¿”ì„œ ì•„ì˜ˆ ìƒˆ ì°½ì„ ë„ì›€)
if "camera_key" not in st.session_state:
    st.session_state.camera_key = 0

st.title("ğŸ“¸ mzêµ¬ë„ ìë™ ì´¬ì˜ê¸° ")
st.info("ì›í•˜ëŠ” ê°ë„ë¥¼ ì„¤ì •í•˜ê³  ì´¬ì˜í•˜ì„¸ìš”!")

# ---------------- 2. ì‚¬ì´ë“œë°” ì„¤ì • ----------------
st.sidebar.header("âš™ï¸ ì„¤ì •")
# ì‚¬ìš©ìê°€ ì„¤ì •í•œ ê°’ì„ Processorë¡œ ë„˜ê²¨ì•¼ í•¨
min_val = st.sidebar.slider("ìµœì†Œ ê°ë„ (Z)", 0.0, 0.5, 0.13, 0.01)
max_val = st.sidebar.slider("ìµœëŒ€ ê°ë„ (Z)", 0.0, 0.5, 0.25, 0.01)

# ---------------- 3. Mediapipe ì´ˆê¸°í™” ----------------
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# ---------------- 4. ì˜ìƒ ì²˜ë¦¬ í´ë˜ìŠ¤ ----------------
class VideoProcessor(VideoProcessorBase):
    def __init__(self):
        self.enter_time = None
        self.capture_triggered = False
        self.last_capture_time = 0
        self.flash_frame = 0
        self.result_queue = queue.Queue()
        
        # [ìˆ˜ì • 2] ì™¸ë¶€ì—ì„œ ì„¤ì •ê°’ì„ ë°›ì„ ë³€ìˆ˜ ì¶”ê°€ (ê¸°ë³¸ê°’ ì„¤ì •)
        self.min_val = 0.02
        self.max_val = 0.20

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        img = cv2.flip(img, 1)
        h, w, _ = img.shape
        
        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb_img)
        
        current_z = 0.0
        in_range = False
        border_color = (0, 0, 255)
        status_msg = "Adjust Angle"
        
        if self.flash_frame > 0:
            self.flash_frame -= 1
            white = np.full((h, w, 3), 255, dtype=np.uint8)
            img = cv2.addWeighted(img, 0.5, white, 0.5, 0)

        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark
            chin = landmarks[152].z
            forehead = landmarks[10].z
            
            # [ìˆ˜ì • 1] * -1 ì œê±° (ì–‘ìˆ˜ ê°’ì´ ë‚˜ì˜¤ë„ë¡)
            current_z = (chin - forehead)
            
            # [ìˆ˜ì • 2] í•˜ë“œì½”ë”© ëŒ€ì‹  self ë³€ìˆ˜ ì‚¬ìš©
            if self.min_val <= current_z <= self.max_val:
                in_range = True
                border_color = (0, 255, 0)
                status_msg = "HOLD ON!"
            
            # í™”ë©´ ê·¸ë¦¬ê¸°
            cv2.rectangle(img, (0,0), (w,h), border_color, 20)
            # ë””ë²„ê¹…ìš©: í˜„ì¬ ì„¤ì • ë²”ìœ„ë„ í™”ë©´ì— í‘œì‹œí•´ì£¼ë©´ ì¢‹ìŒ
            info_text = f"Z: {current_z:.3f} ({self.min_val}~{self.max_val})"
            cv2.putText(img, info_text, (30, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.0, border_color, 2)
            
            if in_range:
                if self.enter_time is None:
                    self.enter_time = time.time()
                
                elapsed = time.time() - self.enter_time
                countdown = 1.5 - elapsed
                
                if countdown > 0:
                    cx, cy = w//2, h//2
                    cv2.putText(img, f"{countdown:.1f}", (cx-50, cy+20), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5)
                else:
                    if not self.capture_triggered:
                        if time.time() - self.last_capture_time > 3:
                            save_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                            self.result_queue.put(save_img)
                            self.last_capture_time = time.time()
                            self.capture_triggered = True
                            self.flash_frame = 5
            else:
                self.enter_time = None
                self.capture_triggered = False
                
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# ---------------- 5. UI ë¡œì§ ----------------

# ì‚¬ì§„ì´ ì°í˜”ì„ ë•Œ
if st.session_state.snapshot is not None:
    st.success("ğŸ“¸ ì¸ìƒìƒ· ê±´ì§!")
    
    # ì´ë¯¸ì§€ í‘œì‹œ
    st.image(st.session_state.snapshot, caption="ê²°ê³¼ë¬¼", use_container_width=True)
    
    # ì €ì¥ ë²„íŠ¼ìš© ì´ë¯¸ì§€ ë³€í™˜
    img_bgr = cv2.cvtColor(st.session_state.snapshot, cv2.COLOR_RGB2BGR)
    is_success, buffer = cv2.imencode(".jpg", img_bgr)
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        if is_success:
            st.download_button(
                label="ğŸ“¥ ì €ì¥í•˜ê¸°",
                data=buffer.tobytes(),
                file_name=f"MZ_Shot_{int(time.time())}.jpg",
                mime="image/jpeg",
                type="primary",
                use_container_width=True
            )
            
    with col2:
        # [ìˆ˜ì • 3] ë‹¤ì‹œ ì°ê¸°: ì¹´ë©”ë¼ í‚¤ë¥¼ ë³€ê²½í•˜ì—¬ ê°•ì œ ë¦¬ë¡œë“œ íš¨ê³¼
        if st.button("ğŸ”„ ë‹¤ì‹œ ì°ê¸° (ìƒˆë¡œê³ ì¹¨)", use_container_width=True):
            st.session_state.snapshot = None
            st.session_state.camera_key += 1 # í‚¤ ë³€ê²½ -> ì»´í¬ë„ŒíŠ¸ ì¬ë§ˆìš´íŠ¸ ìœ ë„
            st.rerun()

# ì´¬ì˜ ëª¨ë“œ (ì‚¬ì§„ì´ ì—†ì„ ë•Œ)
else:
    # [ìˆ˜ì • 3] keyì— ë³€ìˆ˜ë¥¼ ë„£ì–´ ë§¤ë²ˆ ìƒˆë¡œìš´ ì»´í¬ë„ŒíŠ¸ì¸ ê²ƒì²˜ëŸ¼ ì¸ì‹ì‹œí‚´
    dynamic_key = f"mobile-camera-{st.session_state.camera_key}"
    
    ctx = webrtc_streamer(
        key=dynamic_key,
        video_processor_factory=VideoProcessor,
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
        media_stream_constraints={"video": {"facingMode": "user"}, "audio": False},
    )

    # [ìˆ˜ì • 2] ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¬ë¼ì´ë” ê°’ì„ Processorì— ì£¼ì…
    if ctx.video_processor:
        ctx.video_processor.min_val = min_val
        ctx.video_processor.max_val = max_val

    if ctx.state.playing:
        while True:
            if ctx.video_processor:
                try:
                    result_img = ctx.video_processor.result_queue.get(timeout=0.1)
                    if result_img is not None:
                        st.session_state.snapshot = result_img
                        st.rerun()
                except queue.Empty:
                    pass
            time.sleep(0.1)
