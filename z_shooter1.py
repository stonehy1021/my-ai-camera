import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration
import cv2
import mediapipe as mp
import av
import numpy as np
import time
from pathlib import Path

# ---------------- ê¸°ë³¸ ì„¤ì • ----------------
st.set_page_config(page_title="mz êµ¬ë„ ì¹´ë©”ë¼", layout="centered")

# ì €ì¥ ê²½ë¡œ (ì„œë²„ì— ì €ì¥ë¨)
SAVE_DIR = Path("captures")
SAVE_DIR.mkdir(exist_ok=True)

st.title("ğŸ“¸ mzêµ¬ë„ ìë™ ì´¬ì˜ê¸°")
st.info("ì•„ì´í°/ê°¤ëŸ­ì‹œ/PC ëª¨ë‘ ì‘ë™í•©ë‹ˆë‹¤.")

# ---------------- ì‚¬ì´ë“œë°” ì„¤ì • ----------------
st.sidebar.header("âš™ï¸ ì„¤ì •")
# ëª¨ë°”ì¼ í™”ê° íŠ¹ì„±ìƒ Zê°’ ì°¨ì´ê°€ ì‘ê²Œ ë‚˜ì˜¤ë¯€ë¡œ ë²”ìœ„ë¥¼ 0.02~0.15ë¡œ ì¡ìŠµë‹ˆë‹¤.
min_val = st.sidebar.slider("ìµœì†Œ ê°ë„", 0.0, 0.3, 0.02, 0.01)
max_val = st.sidebar.slider("ìµœëŒ€ ê°ë„", 0.0, 0.3, 0.15, 0.01)

# ---------------- Mediapipe ì´ˆê¸°í™” ----------------
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# ---------------- ì˜ìƒ ì²˜ë¦¬ í´ë˜ìŠ¤ (WebRTC) ----------------
class VideoProcessor(VideoProcessorBase):
    def __init__(self):
        self.enter_time = None
        self.capture_triggered = False
        self.last_capture_time = 0
        self.flash_frame = 0

    def recv(self, frame):
        # 1. ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° (ëª¨ë°”ì¼ ì¹´ë©”ë¼ ì˜ìƒ)
        img = frame.to_ndarray(format="bgr24")
        
        # 2. ê±°ìš¸ ëª¨ë“œ (ì¢Œìš° ë°˜ì „)
        img = cv2.flip(img, 1)
        h, w, _ = img.shape
        
        # 3. ì–¼êµ´ ë¶„ì„
        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb_img)
        
        current_z = 0.0
        in_range = False
        border_color = (0, 0, 255) # ë¹¨ê°•
        status_msg = "Adjust Angle"
        
        # 4. í”Œë˜ì‹œ íš¨ê³¼
        if self.flash_frame > 0:
            self.flash_frame -= 1
            # í•˜ì–€ìƒ‰ í™”ë©´ ë®ê¸°
            white = np.full((h, w, 3), 255, dtype=np.uint8)
            img = cv2.addWeighted(img, 0.5, white, 0.5, 0)

        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark
            
            # Z-Diff ê³„ì‚°
            chin = landmarks[152].z
            forehead = landmarks[10].z
            # ëª¨ë°”ì¼/WebRTC í™˜ê²½ ë³´ì •
            current_z = (chin - forehead) * -1 
            
            # ë²”ìœ„ ì²´í¬
            # WebRTC í´ë˜ìŠ¤ ë‚´ë¶€ì—ì„œëŠ” st.session_state ì ‘ê·¼ì´ ê¹Œë‹¤ë¡œì›Œ ê¸°ë³¸ê°’ í˜¹ì€ í•˜ë“œì½”ë”©ëœ ë¡œì§ì„ ì“¸ ìˆ˜ ìˆìœ¼ë‚˜,
            # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ ë„“ì€ ë²”ìœ„(0.02~0.20)ë¥¼ ê¸°ë³¸ ë¡œì§ìœ¼ë¡œ ì¡ìŠµë‹ˆë‹¤.
            # (ì‹¤ì œë¡œëŠ” recv í•¨ìˆ˜ ë°–ì—ì„œ ê°’ì„ ì£¼ì…ë°›ì•„ì•¼ í•˜ì§€ë§Œ, ê°„ë‹¨í•œ êµ¬í˜„ì„ ìœ„í•´ ê³ ì • ë¡œì§ ì‚¬ìš©)
            
            if 0.17 <= current_z <= 0.23: # ëª¨ë°”ì¼ìš© ì¶”ì²œ ë²”ìœ„
                in_range = True
                border_color = (0, 255, 0) # ì´ˆë¡
                status_msg = "HOLD ON!"
            
            # UI ê·¸ë¦¬ê¸°
            cv2.rectangle(img, (0,0), (w,h), border_color, 20)
            
            info_text = f"Z: {current_z:.4f}"
            cv2.putText(img, info_text, (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,0,0), 5)
            cv2.putText(img, info_text, (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.5, border_color, 3)
            
            cv2.putText(img, status_msg, (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)
            
            # ìë™ ì´¬ì˜ ë¡œì§
            if in_range:
                if self.enter_time is None:
                    self.enter_time = time.time()
                
                elapsed = time.time() - self.enter_time
                countdown = 1.5 - elapsed
                
                if countdown > 0:
                    cx, cy = w//2, h//2
                    cv2.putText(img, f"{countdown:.1f}", (cx-50, cy+20), 
                                cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5)
                else:
                    if not self.capture_triggered:
                        if time.time() - self.last_capture_time > 3:
                            # ì €ì¥ (ì„œë²„ì— ì €ì¥ë¨)
                            ts = int(time.time())
                            filename = SAVE_DIR / f"Mobile_Shot_{ts}.jpg"
                            cv2.imwrite(str(filename), img)
                            print(f"ğŸ“¸ ì €ì¥ë¨: {filename}")
                            
                            self.last_capture_time = time.time()
                            self.capture_triggered = True
                            self.flash_frame = 5
            else:
                self.enter_time = None
                self.capture_triggered = False
                
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# ---------------- WebRTC ì‹¤í–‰ ----------------
# ëª¨ë°”ì¼ ì ‘ì†ì„ ìœ„í•œ STUN ì„œë²„ ì„¤ì • (í•„ìˆ˜ - ì´ê±° ì—†ìœ¼ë©´ í°ì—ì„œ ì•ˆë¨)
rtc_config = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

webrtc_streamer(
    key="mobile-camera",
    video_processor_factory=VideoProcessor,
    rtc_configuration=rtc_config,
    media_stream_constraints={
        "video": {"facingMode": "user"}, # ì „ë©´ ì¹´ë©”ë¼ ì‚¬ìš©
        "audio": False
    },
)


