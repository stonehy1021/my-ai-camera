import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase
import cv2
import mediapipe as mp
import av
import numpy as np
import time
import queue

# ---------------- 1. ê¸°ë³¸ ì„¤ì • ----------------
st.set_page_config(page_title="mzêµ¬ë„ ì´¬ì˜ê¸° (ì €ì¥ê°€ëŠ¥)", layout="centered")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "snapshot" not in st.session_state:
    st.session_state.snapshot = None

st.title("ğŸ“¸ mzêµ¬ë„ ìë™ ì´¬ì˜ê¸° ")
st.info("ì„¤ì •í•œ ê°ë„ì— ë§ì¶° ê³ ê°œë¥¼ ë“¤ë©´ 3ì´ˆ ë’¤ ìë™ ì´¬ì˜ë©ë‹ˆë‹¤!")

# ---------------- 2. ì‚¬ì´ë“œë°” ì„¤ì • ----------------
st.sidebar.header("âš™ï¸ ì„¤ì •")
# ì‚¬ìš©ìê°€ ì„¤ì •í•œ ê°’ì„ Processorë¡œ ë„˜ê²¨ì•¼ í•¨
min_val = st.sidebar.slider("ìµœì†Œ ê°ë„ (Z)", 0.0, 0.5, 0.13, 0.01)
max_val = st.sidebar.slider("ìµœëŒ€ ê°ë„ (Z)", 0.0, 0.5, 0.25, 0.01)

# ---------------- 3. Mediapipe ì´ˆê¸°í™” ----------------
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# ---------------- 4. ì˜ìƒ ì²˜ë¦¬ í´ë˜ìŠ¤ ----------------
class VideoProcessor(VideoProcessorBase):
    def __init__(self):
        self.enter_time = None
        self.capture_triggered = False
        self.last_capture_time = 0
        self.flash_frame = 0
        self.result_queue = queue.Queue()
        
        # ì™¸ë¶€ì—ì„œ ì„¤ì •ê°’ì„ ë°›ì„ ë³€ìˆ˜ (ê¸°ë³¸ê°’)
        self.min_val = 0.02
        self.max_val = 0.20

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        img = cv2.flip(img, 1)
        h, w, _ = img.shape
        
        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb_img)
        
        current_z = 0.0
        in_range = False
        border_color = (0, 0, 255) # ë¹¨ê°•
        status_msg = "Adjust Angle"
        
        # í”Œë˜ì‹œ íš¨ê³¼ ë¡œì§
        if self.flash_frame > 0:
            self.flash_frame -= 1
            white = np.full((h, w, 3), 255, dtype=np.uint8)
            img = cv2.addWeighted(img, 0.5, white, 0.5, 0)

        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark
            chin = landmarks[152].z
            forehead = landmarks[10].z
            
            # [ìˆ˜ì •ë¨] * -1 ì œê±°í•˜ì—¬ ì–‘ìˆ˜ ê°’ í™•ë³´
            current_z = (chin - forehead)
            
            # [ìˆ˜ì •ë¨] ìŠ¬ë¼ì´ë” ì„¤ì •ê°’ ì ìš©
            if self.min_val <= current_z <= self.max_val:
                in_range = True
                border_color = (0, 255, 0) # ì´ˆë¡
                status_msg = "HOLD ON!"
            
            # í™”ë©´ ê·¸ë¦¬ê¸°
            cv2.rectangle(img, (0,0), (w,h), border_color, 20)
            
            # ë””ë²„ê¹…ìš©: í˜„ì¬ Zê°’ê³¼ ëª©í‘œ ë²”ìœ„ í‘œì‹œ
            info_text = f"Z: {current_z:.3f} (Goal: {self.min_val}~{self.max_val})"
            cv2.putText(img, info_text, (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, border_color, 2)
            cv2.putText(img, status_msg, (30, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,255), 2)
            
            if in_range:
                if self.enter_time is None:
                    self.enter_time = time.time()
                
                elapsed = time.time() - self.enter_time
                countdown = 1.5 - elapsed
                
                if countdown > 0:
                    cx, cy = w//2, h//2
                    cv2.putText(img, f"{countdown:.1f}", (cx-50, cy+20), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 255), 5)
                else:
                    # â˜… ì´¬ì˜ ì‹œì  â˜…
                    if not self.capture_triggered:
                        if time.time() - self.last_capture_time > 3:
                            save_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                            self.result_queue.put(save_img)
                            self.last_capture_time = time.time()
                            self.capture_triggered = True
                            self.flash_frame = 5
            else:
                self.enter_time = None
                self.capture_triggered = False
                
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# ---------------- 5. UI ë¡œì§ ----------------

# 1. ì‚¬ì§„ì´ ì°íŒ ìƒíƒœ (ê²°ê³¼ í™”ë©´)
if st.session_state.snapshot is not None:
    st.success("ğŸ“¸ ì´¬ì˜ ì™„ë£Œ!")
    
    # ì´ë¯¸ì§€ í‘œì‹œ
    st.image(st.session_state.snapshot, caption="ê²°ê³¼ë¬¼", use_container_width=True)
    
    # ì €ì¥ ë²„íŠ¼ ìƒì„±
    img_bgr = cv2.cvtColor(st.session_state.snapshot, cv2.COLOR_RGB2BGR)
    is_success, buffer = cv2.imencode(".jpg", img_bgr)
    
    if is_success:
        st.download_button(
            label="ğŸ“¥ ì‚¬ì§„ ì €ì¥í•˜ê¸°",
            data=buffer.tobytes(),
            file_name=f"MZ_Shot_{int(time.time())}.jpg",
            mime="image/jpeg",
            type="primary",
            use_container_width=True
        )
    
    # [ë³€ê²½ë¨] ë‹¤ì‹œ ì°ê¸° ë²„íŠ¼ ì‚­ì œí•˜ê³  ì•ˆë‚´ ë¬¸êµ¬ ì¶”ê°€
    st.warning("ğŸ”„ ë‹¤ì‹œ ì´¬ì˜í•˜ì‹œë ¤ë©´ ì›¹í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨ í•´ì£¼ì„¸ìš”.")

# 2. ì´¬ì˜ ëª¨ë“œ (ì‚¬ì§„ì´ ì—†ëŠ” ìƒíƒœ)
else:
    # ì¹´ë©”ë¼ ì‹¤í–‰
    ctx = webrtc_streamer(
        key="mz-camera",
        video_processor_factory=VideoProcessor,
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
        media_stream_constraints={"video": {"facingMode": "user"}, "audio": False},
    )

    # [ì¤‘ìš”] ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¬ë¼ì´ë” ê°’ì„ Processor í´ë˜ìŠ¤ì— ì£¼ì…
    if ctx.video_processor:
        ctx.video_processor.min_val = min_val
        ctx.video_processor.max_val = max_val

    # ì‚¬ì§„ ìˆ˜ì‹  ëŒ€ê¸° ë£¨í”„
    if ctx.state.playing:
        while True:
            if ctx.video_processor:
                try:
                    result_img = ctx.video_processor.result_queue.get(timeout=0.1)
                    if result_img is not None:
                        st.session_state.snapshot = result_img
                        st.rerun() # í™”ë©´ ì „í™˜
                except queue.Empty:
                    pass
            time.sleep(0.1)
